{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALL IMPORTANT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #linear algebra\n",
    "import pandas as pd # data processing, CSV file I/) (e.g. pd.read_cvs)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets as datasets\n",
    "from torchvision import transforms as T # for simplifying the transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell 5480\\Desktop\\DeiT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(data_dir):\n",
    "    all_data = datasets.ImageFolder(data_dir)\n",
    "    return all_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(data_dir, batch_size, train = False):\n",
    "    if train:\n",
    "        #train\n",
    "        transform = T.Compose([   # augmentation transformations occurs here (random horizontal and vertical flips, random color jitter, resizing, center cropping, converting to tensor, normalization (using ImageNet means and standard deviations), and random erasing for regularization.)\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomVerticalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
    "            T.RandomErasing(p=0.2, value='random')\n",
    "        ])\n",
    "        # train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform = transform)\n",
    "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform = transform)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        return train_loader, len(train_data)\n",
    "    else:\n",
    "        # val/test\n",
    "        transform = T.Compose([ # We dont need augmentation for test transforms\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
    "        ])\n",
    "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid/\"), transform=transform)\n",
    "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        return val_loader, test_loader, len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n",
    "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bear', 'Bird', 'Cat', 'Cow', 'Deer', 'Dog', 'Dolphin', 'Elephant', 'Giraffe', 'Horse', 'Kangaroo', 'Lion', 'Panda', 'Tiger', 'Zebra'] 15\n"
     ]
    }
   ],
   "source": [
    "classes = get_classes(\"input/train/\")\n",
    "print(classes, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    \"train\": train_data_len,\n",
    "    \"val\": valid_data_len\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 6 6\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(val_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576 184 184\n"
     ]
    }
   ],
   "source": [
    "print(train_data_len, valid_data_len, test_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, for the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/zipball/main\" to C:\\Users\\Dell 5480/.cache\\torch\\hub\\main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to C:\\Users\\Dell 5480/.cache\\torch\\hub\\checkpoints\\deit_tiny_patch16_224-a1311bcf.pth\n",
      "100%|██████████| 21.9M/21.9M [02:31<00:00, 151kB/s] \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=192, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters(): #freeze model\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = model.head.in_features\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(classes))\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropy() #deals with noisy or uncertain labels in the training data (regularization technique)\n",
    "criterion = criterion.to(device)  #moves the criterion to the specified device (GPU or CPU)\n",
    "optimizer = optim.Adam(model.head.parameters(), lr=0.001)     # used for updating the model parameters.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)  #The learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
    "            if phase == 'train':\n",
    "                model.train() # model to training mode\n",
    "            else:\n",
    "                model.eval() # model to evaluate\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step() # step at end of epoch\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
    "        print()\n",
    "    time_elapsed = time.time() - since # slight error\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:38<00:00, 12.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8293 Acc: 0.6320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:25<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8931 Acc: 0.9348\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:23<00:00, 11.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9325 Acc: 0.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:34<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7516 Acc: 0.9674\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:21<00:00, 10.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8240 Acc: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7157 Acc: 0.9728\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:08<00:00,  9.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7731 Acc: 0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:22<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6779 Acc: 0.9837\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:08<00:00,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7306 Acc: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:23<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6508 Acc: 0.9891\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:26<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6956 Acc: 0.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6465 Acc: 0.9891\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:34<00:00, 11.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6948 Acc: 0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:29<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6357 Acc: 0.9946\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:35<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6786 Acc: 0.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6275 Acc: 0.9946\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:40<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6664 Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6251 Acc: 0.9946\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:30<00:00, 11.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6631 Acc: 0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6211 Acc: 0.9946\n",
      "\n",
      "Training complete in 28m 57s\n",
      "Best Val Acc: 0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler) # now it is a lot faster\n",
    "# I will come back after 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING:\n",
    "\n",
    "Ok, now we finished training. Lets run the dataset on the test loader and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:34<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0795\n",
      "Test Accuracy of  Bear: 100% (12/12)\n",
      "Test Accuracy of  Bird: 100% (11/11)\n",
      "Test Accuracy of   Cat: 100% (11/11)\n",
      "Test Accuracy of   Cow: 100% (12/12)\n",
      "Test Accuracy of  Deer: 100% (10/10)\n",
      "Test Accuracy of   Dog: 100% (10/10)\n",
      "Test Accuracy of Dolphin: 100% ( 6/ 6)\n",
      "Test Accuracy of Elephant: 100% (11/11)\n",
      "Test Accuracy of Giraffe: 100% (11/11)\n",
      "Test Accuracy of Horse: 100% ( 8/ 8)\n",
      "Test Accuracy of Kangaroo: 100% (11/11)\n",
      "Test Accuracy of  Lion: 100% (11/11)\n",
      "Test Accuracy of Panda: 100% (13/13)\n",
      "Test Accuracy of Tiger: 100% (11/11)\n",
      "Test Accuracy of Zebra: 100% (12/12)\n",
      "Test Accuracy of 100% (160/160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0 for i in range(len(classes)))\n",
    "class_total = list(0 for i in range(len(classes)))\n",
    "model_ft.eval()\n",
    "\n",
    "for data, target in tqdm(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    with torch.no_grad(): # turn off autograd for faster testing\n",
    "        output = model_ft(data)\n",
    "        loss = criterion(output, target)\n",
    "    test_loss = loss.item() * data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    if len(target) == 32:\n",
    "        for i in range(32):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss / test_data_len\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
    "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "        ))\n",
    "    else:\n",
    "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
    "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
    "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model earns 93% test accuracy, which is very high. lets save it\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model.cpu(), example)\n",
    "traced_script_module.save(\"animal_classifier.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
